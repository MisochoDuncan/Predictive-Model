# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TWr11CihphoUpSB_iDgrml2XSA_bnOjw
"""

import matplotlib.pyplot as plt #creating static, animated, and interactive visualizations
x = [1, 2, 3, 4, 5, 6, 7, 8, 9]
y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]
y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]
plt.plot(x, y1, label="SEM1")
plt.plot(x, y2, label="SEM2")
plt.plot()
plt.xlabel("X axis")
plt.ylabel("Y axis")
plt.title("DISTRIBUTION PER SEMESTER")
plt.legend() #used to describe elements for a particular area of a graph
plt.show()

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt

def generate_pie_chart(labels, values):
    plt.pie(values, labels=labels, autopct='%1.1f%%')
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.show()

if __name__ == "__main__":
    labels = ["Python", "JavaScript", "C++", "C", "Java"]
    values = [27.9, 25.0, 10.6, 16.3, 20.2]
    generate_pie_chart(labels, values)

import matplotlib.pyplot as plt
x1 = ["BSE","BBIT", "BSCIT", "BCS", "BCM"]
y1 = [200, 400, 250, 180, 180]
plt.bar(x1, y1, label="yellow Bar", color='y')
plt.plot()
plt.xlabel("COURSE")
plt.ylabel("NUMBER OF STUDENTS")
plt.title("INTAKE AS PER THE COURSES")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
 x1 = [2,3,4,5,6]
 y1 = [20, 40, 25, 10, 15]
 x2 = [2, 4, 5, 6, 7]
 y2 = [10, 15, 10, 10, 30]
 plt.bar(x1, y1, label="MANGU CAMPUS", color='y')
 plt.bar(x2, y2, label="TRC CAMPUS", color='g')
 plt.plot()
 plt.xlabel("UNITS REGISTERED")
 plt.ylabel("NUMBER OF STUDENTS")
 plt.title("UNITS REGISTRATION")
 plt.legend()
 plt.show()

# importing the necessary libraries and modules
import matplotlib.pyplot as plt
import numpy as np
# creating the data values for the vertical y and horisontal x axis
x = np.array(["BSE","BBIT", "BSCIT", "BCS", "BCM"])
y = np.array([200, 400, 250, 180, 180])
# using the pyplot.bar funtion
plt.bar(x,y)
plt.plot()
plt.xlabel("COURSE")
plt.ylabel("NUMBER OF STUDENTS")
plt.title("INTAKE AS PER THE COURSES")
# to show our graph
plt.show()

import matplotlib.pyplot as plt
import numpy as np
# Use numpy to generate a bunch of random data in a bell curve around 5.
n = 5 + np.random.randn(1000) #is used to generate an array of random numbers from a standard

m = [m for m in range(len(n))]
plt.bar(m, n)
plt.title("Raw Data")
plt.show()
plt.hist(n, bins=20) #Bins are the number of intervals you want to divide all of your data into, such

#width = (1000 â€“ 0 ) / 20 = 200
plt.title("Histogram")
plt.show()
plt.hist(n, cumulative=True, bins=20, edgecolor="yellow", color="green") #When cumulative is set


plt.title("Cumulative Histogram")
plt.show()

import matplotlib.pyplot as plt
x1 = [2, 3, 4]
y1 = [5, 5, 5]
x2 = [1, 2, 3, 4, 5]
y2 = [2, 3, 2, 3, 4]
y3 = [6, 8, 7, 8, 7]
plt.scatter(x1, y1, label="gw")
plt.scatter(x2, y2, label="geo", marker='v', color='r')
plt.scatter(x2, y3, label="wainaina", marker='^', color='m')
plt.title('Scatter Plot')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
MONTHS = [ 1, 2, 3, 4, 5, 6, 7, 8, 9]
GEORGE = [23, 40, 28, 43, 8, 44, 43, 18, 17]
PETER = [17, 30, 22, 14, 17, 17, 29, 22, 30]
JOYCE = [15, 31, 18, 22, 18, 19, 13, 32, 39]
# Adding legend for stack plots is tricky.
plt.plot([], [], color='r', label = 'GEORGE')
plt.plot([], [], color='g', label = 'PETER')
plt.plot([], [], color='b', label = 'JOYCE')
plt.stackplot(MONTHS, GEORGE, PETER, JOYCE, colors= ['r', 'g', 'b'])
plt.xlabel("MONTH")
plt.ylabel("POINTS")
plt.title('POINTS GARNERED BY MEMBERS')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
labels = 'A', 'B', 'C'
sections = [56, 66, 24]
colors = ['c', 'g', 'y']
plt.pie(sections, labels=labels, colors=colors,startangle=90,explode = (0, 0.1, 0),autopct =
'%1.2f%%')
plt.title('GRADES FOR DATA SCIENCE')
plt.show()

import matplotlib.pyplot as plt

 labels = ["Python", "JavaScript", "C++", "C", "Java"]
 values = [27.9, 25.0, 10.6, 16.3, 20.2]


 plt.pie(values, labels=labels, autopct='%1.2f%%')
 plt.title('Best Programming Languages ')
 plt.show()

import matplotlib.pyplot as plt
x1 = ["1976.0","1980.0", "1984.0", "1988.0", "1992.0", "1996.0", "2000.0", "2004.0", "2008.0"]
y1 = [1300, 1400, 1450, 1650, 1700, 1800, 1980, 1950, 2000]
plt.bar(x1, y1, color = ['b', 'm', 'g', 'r', 'b', 'b', 'b', 'c', 'y'])
plt.plot()
plt.xlabel("Years")
plt.ylabel("No of Athlete")
plt.title("Total Athlete Contribution in Summer Olympics Overtime")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
x1 = ["Utilities","Health Care", "Energy", "Customer Samples", "Communication Services"]
y1 = [70, 450, 390, 120, 120]
plt.barh(x1, y1, color = ['b', 'm', 'g', 'r', 'c',])
plt.plot()
plt.xlabel("Frequency")
plt.ylabel(" Sector ")
plt.title("Total Athlete Contribution in Summer Olympics Overtime")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]
plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)
plt.title("GEORGE")
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import axes3d
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
y1 = np.random.randint(10, size=10)
z1 = np.random.randint(10, size=10)
x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]
y2 = np.random.randint(-10, 0, size=10)
z2 = np.random.randint(10, size=10)
ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')
ax.scatter(x2, y2, z2, c='g', marker='D', label='green')
ax.set_xlabel('x axis')
ax.set_ylabel('y axis')
ax.set_zlabel('z axis')
plt.title("3D Scatter Plot")
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
y = np.random.randint(10, size=10)
z = np.zeros(10)
dx = np.ones(10)
dy = np.ones(10)
dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
ax.bar3d(x, y, z, dx, dy, dz, color='g')
ax.set_xlabel('x axis')
ax.set_ylabel('y axis')
ax.set_zlabel('z axis')
plt.title("3D Bar Chart Example")
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
x, y, z = axes3d.get_test_data()
ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)
plt.title("Duncan")
plt.tight_layout()
plt.show()

import pandas as pd
# Create a DataFrame with sample data
data = {
'Name': ['Peter', 'Joyce', 'George', 'Phylis', 'Moses','Priscillah', 'Eliud'],
'Age': [25, 30, 22, 28, 24, 34, 19],
'Gender': ['Male', 'Female', 'Male', 'Female','Male', 'Female','Male'],
'Marks': [85, 92,'NaN', 88, 'NaN',79,80]
}
wainaina = pd.DataFrame(data)
#wainaina.head(20)
print(wainaina)

import pandas as pd
import numpy as np # Import numpy for numeric operations
# Create a DataFrame with sample data
data = {
'Name': ['Peter', 'Joyce', 'George', 'Phylis', 'Moses','Priscillah', 'Eliud'],
'Age': [25, 30, 22, 28, 24, 34, 19],
'Gender': ['Male', 'Female', 'Male', 'Female','Male', 'Female','Male'],
'Marks': [85, 92,'NaN', 88, 'NaN',79,80]
}
wainaina = pd.DataFrame(data)
# Display the DataFrame
print(wainaina)
c=0
avg=0
sum=0
for me in wainaina['Marks']:
  if str(me).isnumeric():
    c=c+1;
#c+=1
#avg+=me
sum=sum+me
#avg/=c
avg=sum/c;
wainaina1=wainaina.replace(to_replace="NaN",value=avg)
print("\n NEW DATA \n")
print(wainaina1)

# Replace 'Gender' values: 'Female' -> 1.0, 'Male' -> 0.0
wainaina['Gender'] = wainaina['Gender'].replace({'Female': 1.0, 'Male': 0.0})

# Display the updated DataFrame
print(wainaina)

# Replace 'Gender' values: 'Female' -> 1.0, 'Male' -> 0.0
wainaina['Gender'] = wainaina['Gender'].map({'Female': 1.0, 'Male': 0.0}).astype(float)

# Display the updated DataFrame
print(wainaina)

wainaina=wainaina [wainaina ['Marks']=90]
wainaina=wainaina.drop(['Gender'],axis=1)
print(wainaina)

# Sort the DataFrame by 'Marks' column in descending order
wainaina.sort_values(by='Marks', ascending=False, inplace=True)
# Select the top-scoring students (e.g., top 3)
top_scorers = wainaina[['Name', 'Gender', 'Marks']].head(4)
# Display the details of the top-scoring students
print(top_scorers)

import pandas as pd
STUDENTDETAILS = {
'IDNO': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
'NAME': ['Peter', 'Joyce', 'George', 'Phylis', 'Moses','Priscillah', 'Eliud','Veronicah','John', 'Juliet'],
'Campus': ['Main','Ruiru', 'Nairobi', 'Main', 'Ruiru', 'Nairobi','Main', 'Ruiru', 'Nairobi','Main']
}
wainaina1 = pd.DataFrame(STUDENTDETAILS)
print(wainaina1)

import pandas as pd
FEESDETAILS = {
'IDNO': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
'PENDING': ['6000', '375', 'NIL', '7640', '3800','NIL', '1250','900','5200', 'NIL']
}
wainaina2 = pd.DataFrame(FEESDETAILS)
print(wainaina2)

import pandas as pd
STUDENTDETAILS = {
'IDNO': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
'NAME': ['Peter', 'Joyce', 'George', 'Phylis', 'Moses','Priscillah', 'Eliud','Veronicah','John', 'Juliet'],
'Campus': ['Main', 'Ruiru', 'Nairobi', 'Main', 'Ruiru', 'Nairobi','Main', 'Ruiru', 'Nairobi','Main']
}
wainaina1 = pd.DataFrame(STUDENTDETAILS)

FEESDETAILS = {
'IDNO': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
'PENDING': ['6000', '375', 'NIL', '7640', '3800','NIL', '1250','900','5200', 'NIL']
}
wainaina2 = pd.DataFrame(FEESDETAILS)

#merging Dataframe
print(pd.merge(wainaina1,wainaina2,on='IDNO'))

import pandas as pd
# Create a DataFrame for student details
STUDENTDETAILS = {
'IDNO': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
'NAME': ['Peter', 'Joyce', 'George', 'Phylis', 'Moses', 'Priscillah', 'Eliud', 'Veronicah', 'John', 'Juliet'],
'Campus': ['Main', 'Ruiru', 'Nairobi', 'Main', 'Ruiru', 'Nairobi', 'Main', 'Ruiru', 'Nairobi', 'Main']
}
wainaina1 = pd.DataFrame(STUDENTDETAILS)
# Create a DataFrame for student fees
FEESDETAILS = {
'IDNO': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
'PENDING': [6000, 375, 0, 7640, 3800, 0, 1250, 900, 5200, 0]
}
wainaina2 = pd.DataFrame(FEESDETAILS)
# Merge the two DataFrames on 'IDNO' to combine them
merged_df = pd.merge(wainaina1, wainaina2, on='IDNO')
# Group the merged DataFrame by 'Campus' and calculate the total pending fees for each campus
campus_fee_totals = merged_df.groupby('Campus')['PENDING'].sum().reset_index()
# Display the total pending fees for each campus
print(campus_fee_totals)

import pandas as pd
cardata = {
'Brand': ['Maruti', 'Toyota', 'Mahindra', 'Ford', 'Maruti', 'Toyota', 'Toyota', 'Ford', 'Mahindra', 'Maruti'],
'Year': [2009, 2010, 2011, 2010, 2010, 2009, 2010, 2012, 2010, 2009],
'Model': ['Swift', 'Corolla', 'Scorpio', 'Fiesta', 'Alto', 'Camry', 'Innova', 'Figo', 'Bolero', 'WagonR'],
'Sales': [120, 150, 95, 50, 105, 130, 90, 75, 85, 115]
}
# Create a DataFrame from the sample data
wainaina= pd.DataFrame(cardata)
print(wainaina)
print("\n GROUPED DATA \n")
grouped=wainaina.groupby('Year')
# Display the filtered DataFrame for car sales in a given year
print(grouped.get_group(2011))

import pandas as pd
cardata = {
'Brand': ['Maruti', 'Toyota', 'Mahindra', 'Ford', 'Maruti', 'Toyota', 'Toyota', 'Ford', 'Mahindra', 'Maruti'],
'Year': [2009, 2010, 2011, 2010, 2010, 2009, 2010, 2012, 2010, 2009],
'Model': ['Swift', 'Corolla', 'Scorpio', 'Fiesta', 'Alto', 'Camry', 'Innova', 'Figo', 'Bolero', 'WagonR'],
'Sales': [120, 150, 95, 50, 105, 130, 90, 75, 85, 115]
}
# Create a DataFrame from the sample data
wainaina= pd.DataFrame(cardata)
# Filter the data to keep only car sales in the year 2010
sales2010 = wainaina[(wainaina['Year'] == 2010)& (wainaina['Sales'] > 100)]
# Display the filtered DataFrame for car sales in 2010
print(sales2010)

import pandas as pd
# Sample data for car sales
cardata = {
'Brand': ['Maruti', 'Toyota', 'Mahindra', 'Ford', 'Maruti', 'Toyota', 'Toyota', 'Ford', 'Mahindra', 'Maruti'],
'Year': [2009, 2010, 2011, 2010, 2010, 2009, 2010, 2012, 2010, 2009],
'Model': ['Swift', 'Corolla', 'Scorpio', 'Fiesta', 'Alto', 'Camry', 'Innova', 'Figo', 'Bolero', 'WagonR'],
'Sales': [120, 150, 95, 50, 105, 130, 90, 75, 85, 115]
}
# Create a DataFrame from the sample data
wainaina = pd.DataFrame(cardata)
# Filter data for the year 2010 or 2011 using logical OR operator |
filtered_data = wainaina[(wainaina['Year'] == 2010) | (wainaina['Year'] == 2011) | (wainaina['Year'] == 2009)]
# Display the filtered DataFrame
print(filtered_data)

import pandas as pd
wainaina = pd.read_csv('/content/HousingData.csv')
print("Shape of the dataset:", wainaina.shape)
wainaina.head(20)

zu/1195 Wainaina George
4:05 PM
import pandas as pd
wainaina = pd.read_csv('/content/HousingData.csv')
data=wainaina.loc[:,['LSTAT','MEDV']]
data.head(20)

import pandas as pd
import matplotlib.pyplot as plt
wainaina = pd.read_csv('/content/HousingData.csv')
wainaina.plot(x='LSTAT',y='MEDV',style='o')
plt.xlabel('LSTAT')
plt.ylabel('MEDV')
plt.show()

import pandas as pd
wainaina = pd.read_csv('/content/HousingData.csv')
x=pd.DataFrame(data['LSTAT'])
y=pd.DataFrame(data['MEDV'])

import pandas as pd
from sklearn.model_selection import train_test_split
wainaina = pd.read_csv('/content/HousingData.csv')
x=pd.DataFrame(data['LSTAT'])
y=pd.DataFrame(data['MEDV'])
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.linear_model import LinearRegression
regressor=LinearRegression()
regressor.fit(X_train,y_train)

print(regressor.intercept_)

print(regressor.coef_)

y_pred = regressor.predict(X_test)
y_pred

from sklearn import metrics
import numpy as np
# Assuming y_test and y_pred are NumPy arrays or Pandas Series
mae = metrics.mean_absolute_error(y_test, y_pred)
mse = metrics.mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse) # RMSE is the square root of MSE
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
# Load the data
wainaina = pd.read_csv('/content/HousingData.csv')
# Extract the independent variable (X) and dependent variable (y)
X = wainaina[['LSTAT']]
y = wainaina['MEDV']
# Create and fit a linear regression model
regressor = LinearRegression()
regressor.fit(X, y)
# Plot the data points
wainaina.plot(x='LSTAT', y='MEDV', style='o', label='Data Points')
# Overlay the regression line
plt.plot(X, regressor.predict(X), color='red', linewidth=2, label='Regression Line')
plt.xlabel('LSTAT')
plt.ylabel('MEDV')
plt.legend()
plt.show()

import pandas as pd
wainaina = pd.read_csv('/')
print("Shape of the dataset:", wainaina.shape)
wainaina.head(20)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# Load the dataset
wainaina = pd.read_csv('diabetes.csv')
# Split the data into features (X) and labels (y)
X = wainaina.drop('Outcome', axis=1)
y = wainaina['Outcome']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the decision tree classifier
geo= DecisionTreeClassifier(criterion='entropy',random_state=42)
#clf = DecisionTreeClassifier(criterion='gini',random_state=42)
# Train the model on the training data
geo.fit(X_train, y_train)

# Make predictions on the testing set.
y_pred = geo.predict(X_test)
print (y_pred)

# Calculate the accuracy of the decision tree classifier.
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
#Alternatively use the code below:
print(f"Accuracy: {accuracy:.2f}")
# Evaluate the model's performance
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 20))
plot_tree(geo, filled=True, feature_names=X.columns, class_names=["No Diabetes", "Diabetes"])
plt.show()

import joblib
import pandas as pd
# Load the saved model
loaded_model = joblib.load('')
# Prepare your new data with attribute names
new_data = pd.DataFrame({
'Pregnancies': [2, 4, 1, 5],
'Glucose': [100, 150, 95, 130],
'BloodPressure': [70, 80, 60, 90],
'SkinThickness': [32, 35, 25, 40],
'Insulin': [45, 50, 40, 60],
'BMI': [32.0, 35.5, 29.0, 40.2],
'DiabetesPedigreeFunction': [0.4, 0.5, 0.3, 0.6],
'Age': [30, 35, 28, 40]
})
# Make predictions on the new data
predictions = loaded_model.predict(new_data)
# Create a mapping from 0 to "Not Diabetic" and 1 to "Diabetic"
diabetes_mapping = {0: "0-Not Diabetic", 1: "1-Diabetic"}
# Print the predictions with labels
for i, prediction in enumerate(predictions):
 label = diabetes_mapping[prediction]
print(f"Data {i + 1}: Predicted Outcome - {label}")

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Generate synthetic data for illustration
np.random.seed(0)
n_samples = 1000
weather_conditions = np.random.randint(1, 5, size=n_samples)
road_type = np.random.randint(1, 4, size=n_samples)
speed_limit = np.random.randint(20, 70, size=n_samples)
time_of_day = np.random.randint(1, 4, size=n_samples)
vehicle_type = np.random.randint(1, 4, size=n_samples)
driver_age = np.random.randint(18, 75, size=n_samples)
alcohol_involvement = np.random.choice([0, 1], size=n_samples)
seatbelt_usage = np.random.choice([0, 1], size=n_samples)
road_condition = np.random.randint(1, 5, size=n_samples)
severity = np.random.randint(1, 6, size=n_samples)

# Create a DataFrame
data = pd.DataFrame({
    'Weather_Conditions': weather_conditions,
    'Road_Type': road_type,
    'Speed_Limit': speed_limit,
    'Time_of_Day': time_of_day,
    'Vehicle_Type': vehicle_type,
    'Driver_Age': driver_age,
    'Alcohol_Involvement': alcohol_involvement,
    'Seatbelt_Usage': seatbelt_usage,
    'Road_Condition': road_condition,
    'Accident_Severity': severity
})

# Split the data into independent variables (X) and the dependent variable (y)
X = data.drop('Accident_Severity', axis=1)
y = data['Accident_Severity']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Fit the model to the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the model's performance metrics
print("Mean Squared Error: ", mse)
print("R-squared (R2): ", r2)

# Example prediction for a hypothetical set of independent variables
hypothetical_data = np.array([3, 2, 40, 3, 1, 30, 0, 1, 2]).reshape(1, -1)
predicted_severity = model.predict(hypothetical_data)
print("Predicted Accident Severity for Hypothetical Data:", predicted_severity[0])

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Create a hypothetical dataset
data = {
    'VehicleType': [1, 2, 3, 4, 5],
    'DriverAge': [15-20, 21-30, 31-50, 51-70, 71],
    'AlcoholInvolvement': [1, 2, 3, 4, 5],
    'SeatBeltUsage': [1, 2, 3, 4, 5] ,
    'RoadCondition': [1, 2, 3, 4, 5],
    'SpeedLimit': [30, 40, 50, 60, 70],
    'WeatherCondition': [1, 2, 3, 4, 5],
    'Severity': [2, 3, 4, 1, 5]
}

df = pd.DataFrame(data)

# Split the data into training and test sets
X = df[['VehicleType', 'DriverAge','AlcoholInvolvement','SeatBeltUsage','RoadCondition', 'SpeedLimit', 'WeatherCondition']]
y = df['Severity']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Create and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions for a new set of hypothetical independent variables
new_data = pd.DataFrame({'VehicleType': [5], 'DriverAge': [10],'AlcoholInvolvement': [5],'SeatBeltUsage': [5],'RoadCondition': [3], 'SpeedLimit': [60], 'WeatherCondition': [2]})
predicted_severity = model.predict(new_data)
print('The predicted Severity is = ', predicted_severity)

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Generate synthetic data for illustration
np.random.seed(0)
n_samples = 1000
weather_conditions = np.random.randint(1, 5, size=n_samples)
road_type = np.random.randint(1, 4, size=n_samples)
speed_limit = np.random.randint(20, 70, size=n_samples)
time_of_day = np.random.randint(1, 4, size=n_samples)
vehicle_type = np.random.randint(1, 4, size=n_samples)
driver_age = np.random.randint(18, 75, size=n_samples)
alcohol_involvement = np.random.choice([0, 1], size=n_samples)
seatbelt_usage = np.random.choice([0, 1], size=n_samples)
road_condition = np.random.randint(1, 5, size=n_samples)
severity = np.random.randint(1, 6, size=n_samples)

# Create a DataFrame
data = pd.DataFrame({
    'Weather_Conditions': weather_conditions,
    'Road_Type': road_type,
    'Speed_Limit': speed_limit,
    'Time_of_Day': time_of_day,
    'Vehicle_Type': vehicle_type,
    'Driver_Age': driver_age,
    'Alcohol_Involvement': alcohol_involvement,
    'Seatbelt_Usage': seatbelt_usage,
    'Road_Condition': road_condition,
    'Accident_Severity': severity
})

# Split the data into training and test sets
X = data[['Weather_Conditions', 'Road_Type', 'Speed_Limit', 'Time_of_Day', 'Vehicle_Type', 'Driver_Age', 'Alcohol_Involvement', 'Seatbelt_Usage', 'Road_Condition']]
y = data['Accident_Severity']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Create and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions for a hypothetical accident
new_data = pd.DataFrame({
    'Weather_Conditions': [3],
    'Road_Type': [2],
    'Speed_Limit': [50],
    'Time_of_Day': [2],
    'Vehicle_Type': [1],
    'Driver_Age': [30],
    'Alcohol_Involvement': [0],
    'Seatbelt_Usage': [1],
    'Road_Condition': [3]
})

predicted_severity = model.predict(new_data)
print("Predicted Severity for Hypothetical Accident:", predicted_severity[0])

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Creation of a DataFrame you can also add data fron a csv file by importation
data = pd.DataFrame({
    'WeatherConditions': [1, 2, 3, 4, 3, 2, 1, 4, 3, 2],
    'RoadType': [1, 2, 3, 2, 1, 3, 1, 2, 3, 2],
    'SpeedLimit': [30, 40, 50, 60, 70, 40, 50, 60, 30, 70],
    'TimeofDay': [1, 2, 3, 1, 2, 3, 1, 2, 3, 1],
    'VehicleType': [1, 2, 2, 3, 1, 2, 1, 3, 3, 2],
    'DriverAge': [25, 35, 45, 28, 50, 32, 22, 39, 57, 41],
    'AlcoholInvolvement': [0, 0, 1, 0, 0, 1, 0, 1, 0, 0],
    'SeatbeltUsage': [1, 1, 1, 0, 0, 1, 0, 1, 0, 1],
    'RoadCondition': [1, 2, 3, 2, 1, 3, 2, 3, 2, 1],
    'AccidentSeverity': [2, 3, 4, 1, 5, 3, 2, 4, 1, 5]
})

# Splitting the data into training and test sets
X = data[['WeatherConditions', 'RoadType', 'SpeedLimit', 'TimeofDay', 'VehicleType', 'DriverAge', 'AlcoholInvolvement', 'SeatbeltUsage', 'RoadCondition']]
y = data['AccidentSeverity']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Creation and training the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Making predictions for a hypothetical accident
new_data = pd.DataFrame({
    'WeatherConditions': [3],
    'RoadType': [2],
    'SpeedLimit': [50],
    'TimeofDay': [2],
    'VehicleType': [1],
    'DriverAge': [30],
    'AlcoholInvolvement': [0],
    'SeatbeltUsage': [1],
    'RoadCondition': [3]
})

predicted_severity = model.predict(new_data)
print("Predicted Severity for an Accident:", predicted_severity[0])

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# Load the dataset
wainaina = pd.read_csv('diabetes.csv')
# Split the data into features (X) and labels (y)
X = wainaina.drop('Outcome', axis=1)
y = wainaina['Outcome']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize the decision tree classifier
geo= DecisionTreeClassifier(criterion='entropy',random_state=42)
#clf = DecisionTreeClassifier(criterion='gini',random_state=42)
# Train the model on the training data
geo.fit(X_train, y_train)
# Make predictions on the testing set.
y_pred = geo.predict(X_test)
print (y_pred)

# Calculate the accuracy of the decision tree classifier.
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
#Alternatively use the code below:
print(f"Accuracy: {accuracy:.2f}")
# Evaluate the model's performance
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

from google.colab import files
import cv2
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.patches as mpatches
#Upload the image
uploaded = files.upload()
image = cv2.imdecode(np.frombuffer(uploaded[next(iter(uploaded))], np.uint8), -1)
# Load and preprocess the image
#image = cv2.imread('WAINAINA.jpg')
#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
height, width, _ = image.shape
reshaped_image = image.reshape(-1, 3)
# Perform K-Means clustering
k = 6 # Number of clusters
kmeans = KMeans(n_clusters=k, random_state=0).fit(reshaped_image)
labels = kmeans.labels_
centers = kmeans.cluster_centers_
# Create a color-coded segmentation map
segmented_image = centers[labels].reshape(height, width, 3).astype('uint8')
# Create a title for the output
title = f"Image Segmentation with {k} Clusters"
# Define segment labels based on colors
segment_labels = [f"Segment {i+1}" for i in range(k)]
# Visualize the segmented image with title and a legend for colors
plt.figure(figsize=(10, 6))
# Display the segmented image
plt.imshow(segmented_image)
plt.axis('off')
# Add a title using plt.title
plt.title(title, fontsize=16, fontweight='bold')
# Create a legend for colors
legend_patches = [mpatches.Patch(color=centers[i] / 255, label=segment_labels[i]) for i in range(k)]
plt.legend(handles=legend_patches, loc='lower right')
plt.show()

import cv2
import numpy as np
# Load the image
image = cv2.imread('mangu.jpg')
if image is None:
  print("Image not found")
else:
# Convert the image to grayscale for simplicity
  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
# Reshape the image into a flat 1D array for clustering
reshaped_image = gray_image.reshape(-1, 1)
# Perform K-Means clustering to identify the two dominant colors
kmeans = KMeans(n_clusters=2, random_state=0)
kmeans.fit(reshaped_image)
# Find the two cluster centers
cluster_centers = kmeans.cluster_centers_.astype(int)
# Sort cluster centers by color intensity
sorted_cluster_centers = sorted(cluster_centers, key=lambda x: sum(x))
# Define a threshold value to distinguish between the two clusters
threshold = (sorted_cluster_centers[0][0] + sorted_cluster_centers[1][0]) // 2
# Extract the hidden message from pixel values
decoded_bits = [1 if pixel > threshold else 0 for pixel in reshaped_image]
# Reconstruct the message by grouping bits and converting to characters
message_bits = [decoded_bits[i:i+8] for i in range(0, len(decoded_bits), 8)]
decoded_message = ''.join([chr(int(''.join(map(str, bits)), 2)) for bits in message_bits])
print("Decoded Hidden Message:")
print(decoded_message)

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
# Generate synthetic patient data
np.random.seed(0)
n_samples = 200
n_features = 2
# Create two distinct patient clusters
cluster1 = np.random.randn(n_samples // 2, n_features) + np.array([2, 2])
cluster2 = np.random.randn(n_samples // 2, n_features) + np.array([-2, -2])
patient_data = np.vstack([cluster1, cluster2])
# Create a DataFrame for visualization
data_df = pd.DataFrame(patient_data, columns=['Feature1', 'Feature2'])
# Standardize the data
scaler = StandardScaler()
patient_data_std = scaler.fit_transform(patient_data)
# Apply K-Means clustering
n_clusters = 2
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
data_df['Cluster'] = kmeans.fit_predict(patient_data_std)
# Visualize the patient clusters
plt.figure(figsize=(8, 6))
plt.scatter(data_df['Feature1'], data_df['Feature2'], c=data_df['Cluster'], cmap='viridis')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Patient Clusters in Clinical Care')
plt.show()
# Analyze cluster characteristics (e.g., for disease management)
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
print('Cluster Centers:')
print(pd.DataFrame(cluster_centers, columns=['Feature1', 'Feature2']))

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
from google.colab import files
# Upload your dataset (CSV file)
uploaded = files.upload() # Allow the user to upload a CSV dataset file interactively
# Read the uploaded CSV file into a DataFrame
for filename in uploaded.keys():
  data = pd.read_csv(filename)
# Read the uploaded CSV file into a DataFrame
#data = pd.read_csv("insurance.csv")
# Define the feature you want to cluster against 'charges'
feature_to_cluster = 'bmi' # Replace with the feature of your choice
# Convert 'smoker' and 'sex' columns to numerical values
label_encoder = LabelEncoder()
data['smoker'] = label_encoder.fit_transform(data['smoker'])
data['sex'] = label_encoder.fit_transform(data['sex'])
# Create a feature matrix X
X = data[[feature_to_cluster, 'charges']]
# Standardize the data
scaler = StandardScaler()
X_std = scaler.fit_transform(X)
# Apply K-Means clustering
n_clusters = 6 # You can adjust the number of clusters as needed
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
data['Cluster'] = kmeans.fit_predict(X_std)
# Create a title for the output
title = f'Clusters of {feature_to_cluster} against Charges'
# Visualize the clusters with a legend
plt.figure(figsize=(8, 6))
for i in range(n_clusters):
  cluster_data = data[data['Cluster'] == i]
plt.scatter(cluster_data['charges'], cluster_data[feature_to_cluster], c=plt.cm.viridis(i / (n_clusters - 1)), label=f'Cluster {i+1}')
plt.xlabel('Charges')
plt.ylabel(feature_to_cluster)
plt.title(title, fontsize=16, fontweight='bold')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
# Create a dataset of 200 samples and 5 clusters
features, labels = make_blobs(
n_samples=200,
centers=5,
random_state=42 # Add a random state for reproducibility
)
# Instantiate the model with 5 'K' clusters and 10 iterations with different centroid seed
model = KMeans(
n_clusters=5,
n_init=10,
random_state=42
)
# Train the model
model.fit(features)
# Make a prediction on the data
p_labels = model.predict(features)
plt.style.use('default')
# Define a list of colors for the clusters
cluster_colors = ['b', 'g', 'y', 'c', 'm']
# Iterate through the clusters and plot them with different colors and labels
for cluster_num in range(5):
 cluster_data = features[p_labels == cluster_num]
plt.scatter(cluster_data[:, 0], cluster_data[:, 1], alpha=0.8, label=f'Cluster {cluster_num + 1}', c=cluster_colors[cluster_num])
cluster_centers = model.cluster_centers_
cs_x = cluster_centers[:, 0]
cs_y = cluster_centers[:, 1]
# Plot centroids with a different color (e.g., yellow)
plt.scatter(cs_x, cs_y, marker='*', s=300, c='r', label='Centroids')
plt.title('KMeans Clustering')
plt.xlabel('X-axis Label')
plt.ylabel('Y-axis Label')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
# Create a dataset of 200 samples and 5 clusters
features, labels = make_blobs(
n_samples=200,
centers=5
)
# Create an empty list to store WCSS (within-cluster sum of squares) values
wcss = []
# Try different numbers of clusters from 1 to 10
for i in range(1, 11):
 model = KMeans(n_clusters=i, n_init=10, random_state=42)
model.fit(features)
wcss.append(model.inertia_)
# Plot the elbow method graph
plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.grid()
plt.show()

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
wainaina = pd.read_csv(r'/content/Mall_Customers.csv')
print(wainaina) # or use wainaina.head(20)
wainaina.info()
wainaina.duplicated().sum()
wainaina.describe()
print('shape=', wainaina.shape)
print('Number of Duplicated values=',wainaina.duplicated().sum())
wainaina.describe()

import pandas as pd
from sklearn.cluster import KMeans
wainaina = pd.read_csv(r'/content/Mall_Customers.csv')
# Select the numerical columns for clustering (Age, Annual Income, Spending Score)
X = wainaina.iloc[:, [2, 3, 4]].values
# Create an empty list to store WCSS (within-cluster sum of squares) values
wcss = []
# Try different numbers of clusters from 1 to 10
for i in range(1, 11):
 kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
kmeans.fit(X)
wcss.append(kmeans.inertia_)
# Plot the elbow graph
plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.grid()
plt.show()

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.cluster import KMeans

wainaina = pd.read_csv(r'/content/Mall_Customers.csv')

# Specify the optimal number of clusters
optimal_clusters = 5

# User-defined attributes to visualize (you can modify this)
attributes_to_visualize = ['Annual Income (k$)', 'Spending Score (1-100)']

# Select the data for visualization based on user-defined attributes
X = wainaina[attributes_to_visualize].values

# Perform K-Means clustering with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)
y_kmeans = kmeans.fit_predict(X)

# Add the cluster labels to the dataset
wainaina['Cluster'] = y_kmeans

# Specify colors for each cluster (modify this list with your preferred colors)
cluster_colors = ['g', 'y', 'r', 'c', 'm']

# Specify labels for each cluster (modify this list with your preferred labels)
cluster_labels = ['Low Income, Low Spending', 'High Income, Low Spending', 'Moderate Income, Moderate Spending', 'High Income, High Spending', 'Low Income, High Spending']

# Visualize the clusters with specified colors and X symbol for centroids
plt.figure(figsize=(8, 6))
for cluster_num in range(optimal_clusters):
 cluster_data = X[y_kmeans == cluster_num]
plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=cluster_labels[cluster_num], c=cluster_colors[cluster_num])

# Annotate the cluster with the label near its centroid
centroid = kmeans.cluster_centers_[cluster_num]
plt.annotate(cluster_labels[cluster_num], centroid, xytext=(5, 5), textcoords='offset points', fontsize=10)

plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', s=100, c='red', label='Centroids')

# Move the legend outside
plt.legend(loc='upper left', bbox_to_anchor=(1.01, 1))


plt.title('K-Means Clustering')
plt.xlabel(attributes_to_visualize[0]) # Set X-axis label
plt.ylabel(attributes_to_visualize[1]) # Set Y-axis label
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans
import seaborn as sns

# Load the dataset
wainaina = pd.read_csv('/content/Mall_Customers.csv')

# Specify the optimal number of clusters
optimal_clusters = 5

# User-defined attributes to visualize in 3D (you can modify this)
attributes_to_visualize = ['Annual Income (k$)', 'Spending Score (1-100)', 'Age']

# Select the data for visualization based on user-defined attributes
X = wainaina[attributes_to_visualize].values

# Perform K-Means clustering with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)
y_kmeans = kmeans.fit_predict(X)

# Add the cluster labels to the dataset
wainaina['Cluster'] = y_kmeans

# Specify custom colors for each cluster
cluster_colors = ['b', 'g', 'y', 'c', 'k']

# Use Seaborn for color palette
palette = sns.color_palette(cluster_colors)

# Create a 3D scatter plot using Matplotlib and Seaborn for coloring
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

# Add the following two lines to set the azimuth and elevation angles
ax.view_init(elev=20, azim=70) # Adjust the angles as needed

# Specify cluster labels
cluster_labels = ['Low Income, Low Spending', 'High Income, Low Spending', 'Moderate Income, Moderate Spending', 'High Income, High Spending', 'Low Income, High Spending']

for cluster_num in range(optimal_clusters):
 cluster_data = X[y_kmeans == cluster_num]
scatter = ax.scatter(cluster_data[:, 0], cluster_data[:, 1], cluster_data[:, 2], c=palette[cluster_num], s=100, alpha=0.6, label=cluster_labels[cluster_num])

# Add centroids with red "X" symbols
centroids = kmeans.cluster_centers_
ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], c='red', marker='*', s=300, label='Centroids')

# Set labels, title, and color bar
ax.set_xlabel(attributes_to_visualize[0])
ax.set_ylabel(attributes_to_visualize[1])
ax.set_zlabel(attributes_to_visualize[2])
ax.set_title('K-Means Clustering in 3D')

# Move the legend outside
ax.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))

# Show the plot
plt.show()

# Import necessary libraries
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer  # For handling missing values

# Load your dataset
data = pd.read_csv('datamachine.csv')

# Data Preprocessing
# Assuming you have columns: 'timestamp', 'sensor_reading', 'maintenance_event', ...
# You might want to add more sophisticated preprocessing steps based on your specific data.

# Feature Engineering

# Add more feature engineering steps as needed

# Handle missing values
imputer = SimpleImputer(strategy='mean')

# Split the data into training and testing sets


# Define features and target variable
X_train = train_data[['Air Temperature[k]', 'Process temperature [K]', 'Rotational speed [rpm]' , 'Torque [Nm]', 'Tool wear [min]', 'Machine failure']]
y_train = train_data['equipment_failure']

# Initialize the model (you can replace this with the model of your choice)
model = RandomForestClassifier()

# Train the model
model.fit(X_train, y_train)

# Model Evaluation
X_test = test_data[['Air Temperature[k]', 'Process temperature [K]', 'Rotational speed [rpm]' , 'Torque [Nm]', 'Tool wear [min]', 'Machine failure']]
y_test = test_data['equipment_failure']
predictions = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)

print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{report}')

# Proactive Maintenance
# Now, you can use the trained model to predict equipment failures in real-time data
# Implement a system that continuously collects and preprocesses sensor data and uses the model for predictions

# Example:
new_data = pd.read_csv('datamachine.csv')  # Replace with your new data
new_data['rolling_average'] = new_data['sensor_reading'].rolling(window=10).mean()

# Assuming 'new_data' has the same features as your training data
new_predictions = model.predict(new_data[['Air Temperature[k]', 'Process temperature [K]', 'Rotational speed [rpm]' , 'Torque [Nm]', 'Tool wear [min]', 'Machine failure']])

# Implement your proactive maintenance strategy based on new_predictions
# For example, schedule maintenance if the model predicts a high probability of failure
if any(new_predictions == 1):
    print("Proactive Maintenance Scheduled!")
    # Perform proactive maintenance actions here
    # ...

# Regularly update the model using new data and incorporate feedback from maintenance actions
# For simplicity, let's retrain the model every 30 days (adjust as needed)
if pd.to_datetime('today') % pd.DateOffset(days=30) == 0:
    print("Updating Model with New Data...")
    # Load new data
    new_data_for_update = pd.read_csv('new_data_for_update.csv')
    new_data_for_update['rolling_average'] = new_data_for_update['sensor_reading'].rolling(window=10).mean()

    # Assuming 'new_data_for_update' has the same features as your training data
    # Update the model with new data
    X_update = new_data_for_update[['feature1', 'feature2', '...']]
    y_update = new_data_for_update['equipment_failure']

    # Retrain the model
    model.fit(X_update, y_update)

    print("Model Updated Successfully!")

# Implement regulatory updates
# For simplicity, let's assume regulatory updates are manual and occur every 60 days (adjust as needed)
if pd.to_datetime('today') % pd.DateOffset(days=60) == 0:
    print("Regulatory Update: Re-evaluating Model Compliance...")
    # Perform necessary checks for regulatory compliance
    # Update the model or take corrective actions if needed
    # ...

    print("Regulatory Update Complete!")

# Import necessary libraries
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from google.colab import files


uploaded = files.upload()


for filename in uploaded.keys():
  data = pd.read_csv(filename)




train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)


X_train = train_data[['Air Temperature[k]', 'Process temperature [K]', 'Rotational speed [rpm]' , 'Torque [Nm]', 'Tool wear [min]', 'Machine failure''Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Machine failure']]
y_train = train_data['equipment_failure']


model = RandomForestClassifier()


model.fit(X_train, y_train)


X_test = test_data[['Air Temperature[k]', 'Process temperature [K]', 'Rotational speed [rpm]' , 'Torque [Nm]', 'Tool wear [min]', 'Machine failure']]
y_test = test_data['equipment_failure']
predictions = model.predict(X_test)


accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)


print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{report}')


new_data = pd.read_csv('datamachine.csv')
new_data['rolling_average'] = new_data['sensor_reading'].rolling(window=10).mean()



new_predictions = model.predict(new_data[['Air Temperature[k]', 'Process temperature [K]', 'Rotational speed [rpm]' , 'Torque [Nm]', 'Tool wear [min]', 'Machine failure']])

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer  # For handling missing values
import numpy as np

# Generate sample data
np.random.seed(42)  # For reproducibility

timestamps = pd.date_range('2023-01-01', periods=1000, freq='5min')
sensor_readings = np.random.uniform(low=20, high=30, size=(1000, 3))  # Assuming 3 sensor readings
maintenance_events = np.random.choice(['None', 'Scheduled', 'Repair'], size=1000)
equipment_failure = np.random.randint(0, 2, size=1000)

data = pd.DataFrame({
    'timestamp': timestamps,
    'sensor_reading1': sensor_readings[:, 0],
    'sensor_reading2': sensor_readings[:, 1],
    'sensor_reading3': sensor_readings[:, 2],
    'maintenance_event': maintenance_events,
    'equipment_failure': equipment_failure
})

# Data Preprocessing
data['rolling_average'] = data['sensor_reading1'].rolling(window=10).mean()
# Add more feature engineering steps as needed

# Handle missing values
numeric_columns = ['sensor_reading1', 'sensor_reading2', 'sensor_reading3', 'rolling_average']
imputer = SimpleImputer(strategy='mean')

data[numeric_columns] = imputer.fit_transform(data[numeric_columns])

# Now data contains the imputed values for the specified numeric columns

# Split the data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)


# Define features and target variable
X_train = train_data[['sensor_reading1', 'sensor_reading2', 'sensor_reading3', 'rolling_average']]
y_train = train_data['equipment_failure']

# Initialize the model using the logistic regression model
model = LogisticRegression()


# Train the model
model.fit(X_train, y_train)

# Model Evaluation
X_test = test_data[['sensor_reading1', 'sensor_reading2', 'sensor_reading3', 'rolling_average']]
y_test = test_data['equipment_failure']
predictions = model.predict(X_test)

# conduct the evaluation of the model
accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)

print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{report}')

#this is data that has been obtained from a proactive
new_timestamps = pd.date_range('2023-06-01', periods=100, freq='5min')
new_sensor_readings = np.random.uniform(low=20, high=30, size=(100, 3))
new_maintenance_events = np.random.choice(['None', 'Scheduled', 'Repair'], size=100)

new_data = pd.DataFrame({
    'timestamp': new_timestamps,
    'sensor_reading1': new_sensor_readings[:, 0],
    'sensor_reading2': new_sensor_readings[:, 1],
    'sensor_reading3': new_sensor_readings[:, 2],
    'maintenance_event': new_maintenance_events
})

new_data['rolling_average'] = new_data['sensor_reading1'].rolling(window=10).mean()

#predictions on the new data
print(new_data.columns)

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer  # For handling missing values
import numpy as np

# Generating sample data
np.random.seed(42)

timestamps = pd.date_range('2023-01-01', periods=1000, freq='5min')
sensor_readings = np.random.uniform(low=20, high=30, size=(1000, 3))  # Assuming 3 sensor readings are recorded
maintenance_events = np.random.choice(['None', 'Scheduled', 'Repair'], size=1000)
equipment_failure = np.random.randint(0, 2, size=1000)

data = pd.DataFrame({
    'timestamp': timestamps,
    'sensor_reading1': sensor_readings[:, 0],
    'sensor_reading2': sensor_readings[:, 1],
    'sensor_reading3': sensor_readings[:, 2],
    'maintenance_event': maintenance_events,
    'equipment_failure': equipment_failure
})

# Save generated data to a CSV file
data.to_csv('generated_data.csv', index=False)

# Load the generated dataset onto a csv file
data = pd.read_csv('generated_data.csv')

# Data Preprocessing
data['rolling_average'] = data['sensor_reading1'].rolling(window=10).mean()

# Handle missing values
numeric_columns = ['sensor_reading1', 'sensor_reading2', 'sensor_reading3', 'rolling_average']
imputer = SimpleImputer(strategy='mean')

data[numeric_columns] = imputer.fit_transform(data[numeric_columns])

# Split the data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)


# Define features and target variable
X_train = train_data[['sensor_reading1', 'sensor_reading2', 'sensor_reading3', 'rolling_average']]
y_train = train_data['equipment_failure']

# Initialize the model
model = RandomForestClassifier()

# Train the model
model.fit(X_train, y_train)

# Model Evaluation
X_test = test_data[['sensor_reading1', 'sensor_reading2', 'sensor_reading3', 'rolling_average']]
y_test = test_data['equipment_failure']
predictions = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)

print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{report}')

# proactive maintenance from the new data
new_timestamps = pd.date_range('2023-06-01', periods=100, freq='5min')
new_sensor_readings = np.random.uniform(low=20, high=30, size=(100, 3))
new_maintenance_events = np.random.choice(['None', 'Scheduled', 'Repair'], size=100)

new_data = pd.DataFrame({
    'timestamp': new_timestamps,
    'sensor_reading1': new_sensor_readings[:, 0],
    'sensor_reading2': new_sensor_readings[:, 1],
    'sensor_reading3': new_sensor_readings[:, 2],
    'maintenance_event': new_maintenance_events
})

new_data['rolling_average'] = new_data['sensor_reading1'].rolling(window=10).mean()

# predictions on new data
new_data['rolling_average'] = new_data['sensor_reading1'].rolling(window=10).mean()